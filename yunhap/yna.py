import os
import time
import random
import json
import pickle
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from pymongo import MongoClient

# âœ… ë°ì´í„° ì €ì¥ í´ë” ì„¤ì •
DATA_DIR = "../data"
os.makedirs(DATA_DIR, exist_ok=True)

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•¨ìˆ˜
def delete_existing_files():
    files_to_delete = ["latest_news.csv", "latest_news.json", "latest_news.pkl"]
    for file in files_to_delete:
        file_path = os.path.join(DATA_DIR, file)
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ: {file_path}")

# âœ… ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì œê±° í•¨ìˆ˜
def clean_text(text):
    if text:
        text = text.strip()  # ì•ë’¤ ê³µë°± ì œê±°
        text = re.sub(r"\s*\n\s*", " ", text)  # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜ (ë§ˆì¹¨í‘œ ì¶”ê°€ X)
        text = re.sub(r"\s*\t\s*", " ", text)  # íƒ­(\t)ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = re.sub(r"\s+", " ", text)  # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜

        # ë¬¸ì¥ì´ ë§ˆì¹¨í‘œ ì—†ì´ ëë‚œ ê²½ìš°ì—ë§Œ ë§ˆì¹¨í‘œ ì¶”ê°€
        if text and not text.endswith((".", "?", "!", "â€", "\"")):
            text += "."

        # ë”°ì˜´í‘œ(")ê°€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì •ë¦¬
        text = text.replace(" .", ".").replace(" .\"", ".\"")

        return text.strip()
    return None


# âœ… Selenium WebDriver ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920x1080")
chrome_options.add_argument("--disable-dev-shm-usage")

# âœ… WebDriver ì‹¤í–‰
service = Service(ChromeDriverManager().install())
browser = webdriver.Chrome(service=service, options=chrome_options)

# âœ… í¬ë¡¤ë§í•  ê¸°ë³¸ URL
base_url = "https://www.yna.co.kr/economy/real-estate/"
page = 1
all_news = []

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
delete_existing_files()

while True:
    print(f"ğŸ“„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
    url = f"{base_url}{page}?site=wholemenu_economy_depth02"
    browser.get(url)
    wait = WebDriverWait(browser, 15)

    # âœ… JavaScript ì‹¤í–‰í•˜ì—¬ ë™ì  ë¡œë”© ì‹œë„
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(random.uniform(5, 8))

    # âœ… ê¸°ì‚¬ ëª©ë¡ ë¡œë”© ëŒ€ê¸°
    try:
        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')))
        print("âœ… ë‰´ìŠ¤ ì„¹ì…˜ ê°ì§€ ì™„ë£Œ!")
    except:
        print("âš ï¸ ë‰´ìŠ¤ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        break

    # âœ… ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    news_section = browser.find_element(By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')

    # ğŸ”¥ HTML êµ¬ì¡° í™•ì¸ (ë””ë²„ê¹…ìš©)
    html_content = news_section.get_attribute("outerHTML")
    soup = BeautifulSoup(html_content, "html.parser")

    # âœ… ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§
    news_items = []
    articles = soup.select("div.item-box01")

    if not articles:
        print("âš ï¸ 'item-box01' ë‚´ë¶€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„± ìˆìŒ.")

    for article in articles:
        title_tag = article.select_one("a.tit-news span.title01")
        link_tag = article.select_one("a.tit-news")
        date_tag = article.select_one("span.txt-time")
        summary_tag = article.select_one("p.lead")
        image_tag = article.select_one("figure.img-con01 img")

        title = clean_text(title_tag.get_text(strip=True)) if title_tag else None
        link = f"https://www.yna.co.kr{link_tag['href']}" if link_tag and "href" in link_tag.attrs else None
        date = clean_text(date_tag.get_text(strip=True)) if date_tag else None
        summary = clean_text(summary_tag.get_text(strip=True)) if summary_tag else None
        image_url = image_tag["src"] if image_tag and "src" in image_tag.attrs else None

        news_items.append({
            "title": title,
            "link": link,
            "date": date,
            "summary": summary,
            "image_url": image_url
        })

    # âœ… ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    all_news.extend(news_items)

    # âœ… ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    try:
        next_button = browser.find_element(By.XPATH, '//a[@class="next"]')
        next_page_url = next_button.get_attribute("href")

        if not next_page_url:
            print("ğŸšª ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        page += 1
    except:
        print("ğŸšª ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
        break

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(all_news)

# âœ… CSV ì €ì¥ (MongoDB ë° SQL Import-Friendly)
csv_path = os.path.join(DATA_DIR, "latest_news.csv")
df.to_csv(csv_path, index=False, encoding="utf-8-sig", na_rep="NULL", quotechar='"', doublequote=True)

# âœ… JSON ì €ì¥ (MongoDB Import-Friendly)
json_path = os.path.join(DATA_DIR, "latest_news.json")
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=4)

# âœ… Pickle ì €ì¥ (Python ê°ì²´ ê·¸ëŒ€ë¡œ ì €ì¥, ì„±ëŠ¥ ìµœì í™”)
pkl_path = os.path.join(DATA_DIR, "latest_news.pkl")
with open(pkl_path, "wb") as f:
    pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)

# âœ… MongoDB ì €ì¥
mongo_client = MongoClient("mongodb://localhost:27017/")
mongo_db = mongo_client["news_db"]
mongo_collection = mongo_db["latest_news"]

# âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
try:
    mongo_collection.delete_many({})
    print("ğŸ—‘ MongoDB ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
except Exception as e:
    print(f"âš ï¸ MongoDB ë°ì´í„° ì‚­ì œ ì˜¤ë¥˜ ë°œìƒ: {e}")

# âœ… ìƒˆ ë°ì´í„° ì‚½ì…
try:
    mongo_collection.insert_many(df.to_dict(orient="records"))
    print("âœ… MongoDB ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
except Exception as e:
    print(f"âš ï¸ MongoDB ë°ì´í„° ì‚½ì… ì˜¤ë¥˜ ë°œìƒ: {e}")

print(f"âœ… ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ“‚ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
print(f"ğŸ“‚ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_path}")
print(f"ğŸ“‚ Pickle íŒŒì¼ ì €ì¥ ì™„ë£Œ: {pkl_path}")
print("ğŸ“‚ MongoDB ì €ì¥ ì™„ë£Œ!")

# âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ
browser.quit()
print("ğŸšª ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ!")

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„° í™•ì¸
if not df.empty:
    print("ğŸ” í¬ë¡¤ë§ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
else:
    print("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

import os
import time
import random
import json
import pickle
import pandas as pd
import re
import numpy as np
from pymongo import MongoClient
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from konlpy.tag import Mecab
from sentence_transformers import SentenceTransformer

# âœ… MongoDB ì—°ê²° ì„¤ì •
MONGO_URI = "mongodb://eunbikang:1234@localhost:27017/admin"
client = MongoClient(MONGO_URI)
db = client["admin"]
collection = db["latest_news"]

# âœ… SentenceTransformer ëª¨ë¸ ë¡œë“œ (ë²¡í„°í™”)
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# âœ… Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ
mecab = Mecab()

# âœ… ë°ì´í„° ì €ì¥ í´ë” ì„¤ì •
DATA_DIR = "../data"
os.makedirs(DATA_DIR, exist_ok=True)

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•¨ìˆ˜
def delete_existing_files():
    files_to_delete = ["latest_news.csv", "latest_news.json", "latest_news.pkl"]
    for file in files_to_delete:
        file_path = os.path.join(DATA_DIR, file)
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ: {file_path}")

# âœ… ID ìë™ ì¦ê°€ í•¨ìˆ˜ (MongoDBì—ì„œ ê°€ì¥ í° id ê°’ ì°¾ê¸°)
def get_next_id():
    last_doc = collection.find_one(sort=[("id", -1)])  # ê°€ì¥ í° id ì°¾ê¸°
    return last_doc["id"] + 1 if last_doc else 1  # ë°ì´í„° ì—†ìœ¼ë©´ 1ë¶€í„° ì‹œì‘

# âœ… í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜ (ì¤„ë°”ê¿ˆ, ê³µë°±, ë§ˆì¹¨í‘œ ì²˜ë¦¬)
def clean_text(text):
    if text:
        text = text.strip()
        text = re.sub(r"\s*\n\s*", " ", text)  # ê°œí–‰ â†’ ê³µë°± ë³€í™˜
        text = re.sub(r"\s*\t\s*", " ", text)  # íƒ­ â†’ ê³µë°± ë³€í™˜
        text = re.sub(r"\s+", " ", text)  # ì—°ì†ëœ ê³µë°± ì••ì¶•

        # ë¬¸ì¥ì´ ë§ˆì¹¨í‘œ ì—†ì´ ëë‚˜ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
        if text and not text.endswith((".", "?", "!", "â€", "\"")):
            text += "."

        text = text.replace(" .", ".").replace(" .\"", ".\"")

        return text.strip()
    return None

# âœ… í˜•íƒœì†Œ ë¶„ì„ (í‚¤ì›Œë“œ ì¶”ì¶œ)
def extract_keywords(text):
    tokens = mecab.nouns(text)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ
    return " ".join(tokens)

# âœ… ë²¡í„°í™” í•¨ìˆ˜ (ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜)
def vectorize_text(text):
    return model.encode(text).tolist()

# âœ… ì—°í•©ë‰´ìŠ¤(ë¶€ë™ì‚°) ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news():
    print("ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    # âœ… Selenium WebDriver ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # âœ… WebDriver ì‹¤í–‰
    service = Service(ChromeDriverManager().install())
    browser = webdriver.Chrome(service=service, options=chrome_options)

    # âœ… í¬ë¡¤ë§í•  ê¸°ë³¸ URL
    base_url = "https://www.yna.co.kr/economy/real-estate/"
    page = 1
    all_news = []

    while page <= 2:  # âœ… 2í˜ì´ì§€ê¹Œì§€ë§Œ í¬ë¡¤ë§ (ë” ëŠ˜ë¦´ ìˆ˜ë„ ìˆìŒ)
        print(f"ğŸ“„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        url = f"{base_url}{page}?site=wholemenu_economy_depth02"
        browser.get(url)
        wait = WebDriverWait(browser, 15)

        browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(5, 8))

        try:
            wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')))
            print("âœ… ë‰´ìŠ¤ ì„¹ì…˜ ê°ì§€ ì™„ë£Œ!")
        except:
            print("âš ï¸ ë‰´ìŠ¤ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
            break

        news_section = browser.find_element(By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')
        html_content = news_section.get_attribute("outerHTML")
        soup = BeautifulSoup(html_content, "html.parser")

        articles = soup.select("div.item-box01")
        if not articles:
            print("âš ï¸ 'item-box01' ë‚´ë¶€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„± ìˆìŒ.")

        for article in articles:
            title_tag = article.select_one("a.tit-news span.title01")
            link_tag = article.select_one("a.tit-news")
            date_tag = article.select_one("span.txt-time")
            summary_tag = article.select_one("p.lead")
            image_tag = article.select_one("figure.img-con01 img")

            title = clean_text(title_tag.get_text(strip=True)) if title_tag else None
            link = f"https://www.yna.co.kr{link_tag['href']}" if link_tag and "href" in link_tag.attrs else None
            date = clean_text(date_tag.get_text(strip=True)) if date_tag else None
            summary = clean_text(summary_tag.get_text(strip=True)) if summary_tag else None
            image_url = image_tag["src"] if image_tag and "src" in image_tag.attrs else None

            if summary:
                keywords = extract_keywords(summary)  # âœ… í‚¤ì›Œë“œ ì¶”ì¶œ
                vector = vectorize_text(keywords)  # âœ… ë²¡í„°í™”

                news_data = {
                    "id": get_next_id(),  # âœ… ìë™ ì¦ê°€ ID ì¶”ê°€
                    "title": title,
                    "link": link,
                    "date": date,
                    "summary": summary,
                    "image_url": image_url,
                    "vector": vector
                }

                all_news.append(news_data)

        page += 1

    browser.quit()
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")

    return all_news

# âœ… MongoDBì— ë°ì´í„° ì €ì¥
def save_to_mongodb(news_list):
    if not news_list:
        print("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    # âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
    collection.delete_many({})
    print("ğŸ—‘ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")

    # âœ… ìƒˆ ë°ì´í„° ì €ì¥
    collection.insert_many(news_list)
    print(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ!")

# âœ… ì‹¤í–‰ (í¬ë¡¤ë§ â†’ ë²¡í„°í™” â†’ MongoDB ì €ì¥)
if __name__ == "__main__":
    delete_existing_files()
    news_data = crawl_news()
    save_to_mongodb(news_data)

    # âœ… MongoDB ë°ì´í„° í™•ì¸
    doc_count = collection.count_documents({})
    print(f"ğŸ” MongoDB ì €ì¥ëœ ë‰´ìŠ¤ ê°œìˆ˜: {doc_count}")

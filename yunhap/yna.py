import os
import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager

# âœ… ë°ì´í„° ì €ì¥ í´ë” ì„¤ì •
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# âœ… Selenium WebDriver ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920x1080")
chrome_options.add_argument("--disable-dev-shm-usage")

# âœ… WebDriver ì‹¤í–‰
service = Service(ChromeDriverManager().install())
browser = webdriver.Chrome(service=service, options=chrome_options)

# âœ… í¬ë¡¤ë§í•  URL
url = "https://www.yna.co.kr/economy/real-estate?site=wholemenu_economy_depth02"
browser.get(url)
wait = WebDriverWait(browser, 15)  # ìµœëŒ€ 15ì´ˆê¹Œì§€ ëŒ€ê¸°

# âœ… JavaScript ì‹¤í–‰í•˜ì—¬ ë™ì  ë¡œë”© ì‹œë„ (ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°)
browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")  # ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
time.sleep(random.uniform(5, 8))  # âœ… JavaScript ì‹¤í–‰ í›„ ëŒ€ê¸°

# âœ… ì •í™•í•œ XPathë¥¼ ì‚¬ìš©í•˜ì—¬ `container512` ì°¾ê¸°
try:
    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="container"]/div[2]')))
    print("âœ… ë‰´ìŠ¤ ì„¹ì…˜ ê°ì§€ ì™„ë£Œ!")
except:
    print("âš ï¸ ë‰´ìŠ¤ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„± ìˆìŒ.")
    browser.quit()
    exit()

# âœ… `container512` ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
news_section = browser.find_element(By.XPATH, '//*[@id="container"]/div[2]')

# âœ… BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
soup = BeautifulSoup(news_section.get_attribute("outerHTML"), "html.parser")

# âœ… ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§
news_items = []
articles = soup.find_all("li")  # ë‰´ìŠ¤ ëª©ë¡
for article in articles:
    title_tag = article.find("a", class_="tit-news")
    date_tag = article.find("span", class_="txt-time")
    summary_tag = article.find("p", class_="lead")

    title = title_tag.get_text(strip=True) if title_tag else None
    link = title_tag["href"] if title_tag and "href" in title_tag.attrs else None
    date = date_tag.get_text(strip=True) if date_tag else None
    summary = summary_tag.get_text(strip=True) if summary_tag else None

    news_items.append({
        "title": title,
        "link": f"https://www.yna.co.kr{link}" if link else None,
        "date": date,
        "summary": summary
    })

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(news_items)

# âœ… CSV ì €ì¥
csv_path = os.path.join(DATA_DIR, "latest_news.csv")
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# âœ… PKL ì €ì¥
pkl_path = os.path.join(DATA_DIR, "latest_news.pkl")
df.to_pickle(pkl_path)

print(f"âœ… ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ“‚ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
print(f"ğŸ“‚ PKL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {pkl_path}")

# âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ
browser.quit()
print("ğŸšª ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ!")

import os
import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager

# âœ… ë°ì´í„° ì €ì¥ í´ë” ì„¤ì • (../data)
DATA_DIR = "../data"
os.makedirs(DATA_DIR, exist_ok=True)

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•¨ìˆ˜
def delete_existing_files():
    csv_path = os.path.join(DATA_DIR, "latest_news.csv")
    pkl_path = os.path.join(DATA_DIR, "latest_news.pkl")

    if os.path.exists(csv_path):
        os.remove(csv_path)
        print(f"ğŸ—‘ ê¸°ì¡´ CSV íŒŒì¼ ì‚­ì œ: {csv_path}")

    if os.path.exists(pkl_path):
        os.remove(pkl_path)
        print(f"ğŸ—‘ ê¸°ì¡´ PKL íŒŒì¼ ì‚­ì œ: {pkl_path}")

# âœ… Selenium WebDriver ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920x1080")
chrome_options.add_argument("--disable-dev-shm-usage")

# âœ… WebDriver ì‹¤í–‰
service = Service(ChromeDriverManager().install())
browser = webdriver.Chrome(service=service, options=chrome_options)

# âœ… í¬ë¡¤ë§í•  ê¸°ë³¸ URL
base_url = "https://www.yna.co.kr/economy/real-estate/"
page = 1  # ì‹œì‘ í˜ì´ì§€
all_news = []  # ì „ì²´ ë‰´ìŠ¤ ì €ì¥ ë¦¬ìŠ¤íŠ¸

# âœ… ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
delete_existing_files()

while True:
    print(f"ğŸ“„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
    url = f"{base_url}{page}?site=wholemenu_economy_depth02"
    browser.get(url)
    wait = WebDriverWait(browser, 15)

    # âœ… JavaScript ì‹¤í–‰í•˜ì—¬ ë™ì  ë¡œë”© ì‹œë„
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")  # ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
    time.sleep(random.uniform(5, 8))  # âœ… JavaScript ì‹¤í–‰ í›„ ëŒ€ê¸°

    # âœ… ê¸°ì‚¬ ëª©ë¡ì´ ìˆëŠ” `list01`ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    try:
        wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')))
        print("âœ… ë‰´ìŠ¤ ì„¹ì…˜ ê°ì§€ ì™„ë£Œ!")
    except:
        print("âš ï¸ ë‰´ìŠ¤ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        break

    # âœ… ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (item-box01 ë‚´ë¶€ì˜ ë°ì´í„° í¬ë¡¤ë§)
    news_section = browser.find_element(By.XPATH, '//*[@id="container"]/div[2]/div[2]/div[1]/section/div/ul')

    # ğŸ”¥ HTML êµ¬ì¡° í™•ì¸ (ë””ë²„ê¹…ìš©)
    html_content = news_section.get_attribute("outerHTML")
    print("\nğŸ” í˜„ì¬ ë‰´ìŠ¤ ì„¹ì…˜ HTML êµ¬ì¡°:\n", html_content[:1000])  # âœ… HTML ì¼ë¶€ í™•ì¸ (1000ì ì œí•œ)

    soup = BeautifulSoup(html_content, "html.parser")

    # âœ… ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§
    news_items = []
    articles = soup.select("div.item-box01")  # âœ… ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸

    if not articles:
        print("âš ï¸ 'item-box01' ë‚´ë¶€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„± ìˆìŒ.")

    for article in articles:
        title_tag = article.select_one("a.tit-news span.title01")  # âœ… ê¸°ì‚¬ ì œëª©
        link_tag = article.select_one("a.tit-news")  # âœ… ê¸°ì‚¬ ë§í¬
        date_tag = article.select_one("span.txt-time")  # âœ… ë‚ ì§œ
        summary_tag = article.select_one("p.lead")  # âœ… ìš”ì•½

        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
        link = link_tag["href"] if link_tag and "href" in link_tag.attrs else "ë§í¬ ì—†ìŒ"
        date = date_tag.get_text(strip=True) if date_tag else "ë‚ ì§œ ì—†ìŒ"
        summary = summary_tag.get_text(strip=True) if summary_tag else "ìš”ì•½ ì—†ìŒ"

        news_items.append({
            "title": title,
            "link": f"https://www.yna.co.kr{link}" if "ë§í¬ ì—†ìŒ" not in link else None,
            "date": date,
            "summary": summary
        })

    # âœ… ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    all_news.extend(news_items)

    # âœ… ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    try:
        next_button = browser.find_element(By.XPATH, '//a[@class="next"]')
        next_page_url = next_button.get_attribute("href")

        if not next_page_url:
            print("ğŸšª ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break  # ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

        page += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
    except:
        print("ğŸšª ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
        break

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(all_news)

# âœ… CSV ì €ì¥
csv_path = os.path.join(DATA_DIR, "latest_news.csv")
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# âœ… PKL ì €ì¥
pkl_path = os.path.join(DATA_DIR, "latest_news.pkl")
df.to_pickle(pkl_path)

print(f"âœ… ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ“‚ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
print(f"ğŸ“‚ PKL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {pkl_path}")

# âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ
browser.quit()
print("ğŸšª ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ!")

# âœ… í¬ë¡¤ë§ëœ ë°ì´í„° í™•ì¸ (ì¼ë°˜ Python í™˜ê²½ì—ì„œë„ ê°€ëŠ¥)
if not df.empty:
    print("ğŸ” í¬ë¡¤ë§ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())  # âœ… ì¼ë°˜ Python í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
else:
    print("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

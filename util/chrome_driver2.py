import sys
import time
import random
import pandas as pd
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By

# ğŸ”¥ sys.path ì¶”ê°€í•˜ì—¬ crawling ê²½ë¡œì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
sys.path.append("..")

# ğŸ”¥ ì˜¬ë°”ë¥¸ ê²½ë¡œì—ì„œ driver ê°€ì ¸ì˜¤ê¸°
from dynamic_crawling import driver

def scrape_page_data(browser):
    """ í˜„ì¬ í˜ì´ì§€ì˜ ë§¤ì¥ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ """
    trs = BeautifulSoup(browser.page_source, 'html.parser').find_all("tr")
    page_data = []

    for row in trs:
        cols = row.find_all("td")
        if len(cols) >= 6:
            data = {
                "ì§€ì—­": cols[0].text.strip(),
                "ë§¤ì¥ëª…": cols[1].text.strip(),
                "í˜„í™©": cols[2].text.strip(),
                "ì£¼ì†Œ": cols[3].text.strip(),
                "ì„œë¹„ìŠ¤": ", ".join(img["alt"] for img in cols[4].find_all("img")),
                "ì „í™”ë²ˆí˜¸": cols[5].text.strip()
            }
            page_data.append(data)

    return page_data

if __name__ == "__main__":
    url = "https://www.hollys.co.kr/store/korea/korStore2.do?a=2&b=3"
    browser = driver()  # âœ… driver() í•¨ìˆ˜ í˜¸ì¶œ
    browser.get(url)
    time.sleep(random.uniform(2, 4))

    # âœ… í¬ë¡¤ë§í•  ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
    all_data = []
    total_pages = 49  # ğŸ”¥ ì „ì²´ í˜ì´ì§€ ìˆ˜
    page = 1

    while page <= total_pages:
        try:
            print(f"ğŸ”„ {page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

            # âœ… í˜„ì¬ í˜ì´ì§€ í¬ë¡¤ë§
            page_data = scrape_page_data(browser)
            if not page_data:
                print(f"â›” {page} í˜ì´ì§€ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break

            all_data.extend(page_data)

            time.sleep(random.uniform(2, 4))  # âœ… ëŒ€ê¸° ì‹œê°„ ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)

            # âœ… ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            if page % 10 == 0 and page < total_pages:
                # ğŸ”¥ 'ë‹¤ìŒ10ê°œ' ë²„íŠ¼ í´ë¦­
                try:
                    next_button = browser.find_element(By.XPATH, "//a[contains(@onclick, 'paging')]/img[@alt='ë‹¤ìŒ10ê°œ']/parent::a")
                    next_button.click()
                    print(f"â¡ï¸ 'ë‹¤ìŒ10ê°œ' ë²„íŠ¼ í´ë¦­í•˜ì—¬ ìƒˆë¡œìš´ í˜ì´ì§€ ê·¸ë£¹ ë¡œë“œ")
                except:
                    print("â›” 'ë‹¤ìŒ10ê°œ' ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break
            else:
                # ğŸ”¥ ê°œë³„ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
                try:
                    next_page = browser.find_element(By.XPATH, f"//a[contains(@onclick, 'paging({page + 1})')]")
                    next_page.click()
                    print(f"â¡ï¸ {page + 1} í˜ì´ì§€ë¡œ ì´ë™")
                except:
                    print(f"â›” {page + 1} í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break

            page += 1
            time.sleep(random.uniform(2, 4))  # âœ… í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        except Exception as e:
            print(f"â›” í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            break  # ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # âœ… í¬ë¡¤ë§ ì™„ë£Œëœ ë°ì´í„° CSVë¡œ ì €ì¥
    df = pd.DataFrame(all_data)
    df.to_csv("hollys_stores.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… ì´ {len(all_data)}ê°œ ë§¤ì¥ ë°ì´í„° ì €ì¥ ì™„ë£Œ! (CSV)")

    # âœ… í¬ë¡¤ë§ ë°ì´í„°ë¥¼ `.pkl` íŒŒì¼ë¡œ ì €ì¥
    df.to_pickle("hollys_stores.pkl")
    print(f"âœ… ì´ {len(all_data)}ê°œ ë§¤ì¥ ë°ì´í„° ì €ì¥ ì™„ë£Œ! (PKL)")

    browser.quit()  # âœ… ë¸Œë¼ìš°ì € ë‹«ê¸° ì¶”ê°€
    print("ğŸšª ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ!")

import os
import requests
from bs4 import BeautifulSoup
import pandas as pd

# ì €ì¥í•  í´ë” ì§€ì •
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# ê¸°ë³¸ URL ì„¤ì •
BASE_URL = "https://www.hollys.co.kr/store/korea/korStore2.do"

# User-Agent ì„¤ì • (ì›¹ì‚¬ì´íŠ¸ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

def get_store_data(page: int) -> list:
    """ íŠ¹ì • í˜ì´ì§€ì˜ ë§¤ì¥ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ """
    url = f"{BASE_URL}?pageNo={page}&sido=&gugun=&store="

    # ì›¹ í˜ì´ì§€ ìš”ì²­ (User-Agent ì¶”ê°€)
    response = requests.get(url, headers=HEADERS)

    # ì‘ë‹µ ì½”ë“œ í™•ì¸
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (í˜ì´ì§€ {page}, ìƒíƒœ ì½”ë“œ: {response.status_code})")
        return None

    # HTML íŒŒì‹±
    response.encoding = "utf-8"
    soup = BeautifulSoup(response.text, "html.parser")

    # âœ… ë³€ê²½ëœ í…Œì´ë¸” ì„ íƒì ì ìš©
    table = soup.select_one("table.tb_store")
    if not table:
        print(f"âŒ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (í˜ì´ì§€ {page})")
        return None

    rows = table.select("tbody tr")

    # âœ… ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì¤‘ë‹¨
    if not rows:
        print(f"ğŸš« í˜ì´ì§€ {page}ì— ë” ì´ìƒ ë§¤ì¥ì´ ì—†ìŒ, í¬ë¡¤ë§ ì¤‘ë‹¨!")
        return None

    # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì´ìš©í•œ ìµœì í™”ëœ í¬ë¡¤ë§
    stores = [
        {
            "ì§€ì—­": cols[0].text.strip(),
            "ë§¤ì¥ëª…": cols[1].text.strip(),
            "í˜„í™©": cols[2].text.strip(),
            "ì£¼ì†Œ": cols[3].text.strip(),
            "ì„œë¹„ìŠ¤": ", ".join(img["alt"] for img in cols[4].find_all("img")),
            "ì „í™”ë²ˆí˜¸": cols[5].text.strip(),
        }
        for row in rows if (cols := row.find_all("td")) and len(cols) >= 6
    ]

    # âœ… ë§¤ì¥ ë°ì´í„°ê°€ 0ê°œë©´ í¬ë¡¤ë§ ì¤‘ë‹¨
    if len(stores) == 0:
        print(f"ğŸš« í˜ì´ì§€ {page}ì— ë§¤ì¥ì´ ì—†ìŒ, í¬ë¡¤ë§ ì¤‘ë‹¨!")
        return None

    return stores


def scrape_all_pages():
    """ ëª¨ë“  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  í•˜ë‚˜ì˜ CSV ë° PKL íŒŒì¼ì— ì €ì¥ """
    all_stores = []
    page = 1

    while True:
        stores = get_store_data(page)

        # âœ… ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if stores is None:
            print(f"âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ {page - 1}ê¹Œì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
            break

        all_stores.extend(stores)
        print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ ({len(stores)}ê°œ ë§¤ì¥)")
        page += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™

    # ì „ì²´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ CSV ë° PKL íŒŒì¼ë¡œ ì €ì¥
    if all_stores:
        df_total = pd.DataFrame(all_stores)
        df_total.to_csv(os.path.join(DATA_DIR, "hollys_stores.csv"), index=False, encoding="utf-8-sig")
        df_total.to_pickle(os.path.join(DATA_DIR, "hollys_stores.pkl"))
        print(f"ğŸ“ ì „ì²´ ë°ì´í„° ì €ì¥ ì™„ë£Œ (ì´ {len(all_stores)}ê°œ ë§¤ì¥)")

if __name__ == "__main__":
    scrape_all_pages()
